

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="学习《吴恩达机器学习》的笔记">
  <meta name="author" content="Mihao">
  <meta name="keywords" content="吴恩达,机器学习">
  <meta name="description" content="学习《吴恩达机器学习》的笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达——机器学习">
<meta property="og:url" content="https://mihaoyoung.github.io/91a709bb/">
<meta property="og:site_name" content="养羊小站">
<meta property="og:description" content="学习《吴恩达机器学习》的笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306142126309.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306163104402.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210307131430617.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316161356522.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316172644451.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316172711040.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210324100709079.png">
<meta property="og:image" content="https://mihaoyoung.github.io/91a709bb/image-20210324101135246.png">
<meta property="article:published_time" content="2021-03-05T14:16:50.000Z">
<meta property="article:modified_time" content="2021-03-24T02:43:41.120Z">
<meta property="article:author" content="Mihao">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306142126309.png">
  
  <title>吴恩达——机器学习 - 养羊小站</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"mihaoyoung.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="吴恩达——机器学习">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-03-05 22:16" pubdate>
        2021年3月5日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      10k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      31 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">吴恩达——机器学习</h1>
            
            <div class="markdown-body">
              <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.                                                                ——Tom Mitchell</p>
</blockquote>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs text">Example: playing checkers.<br><br>E = the experience of playing many games of checkers<br><br>T = the task of playing checkers.<br><br>P = the probability that the program will win the next game.<br></code></pre></td></tr></table></figure>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><blockquote>
<p>得到一个数据集，知道<strong>正确</strong>的输出数据，并且认为输入和输出数据之间<strong>存在某种关系</strong>，</p>
<p>即，<strong>在知道输入和输出的情况下训练出一个模型，将输入映射到输出</strong></p>
</blockquote>
<p>流程</p>
<ul>
<li>选择一个适合目标任务的数学模型</li>
<li>先把一部分已知的“问题和答案”（训练集）给机器去学习</li>
<li>机器总结出了自己的“方法论”</li>
<li>人类把”新的问题”（测试集）给机器，让他去解答</li>
</ul>
<p>监督学习分为两大问题，”<strong>regression</strong>“和”<strong>classification</strong>“</p>
<blockquote>
<p>回归问题，利用<strong>连续</strong>的输出数据来预测结果，得到的是一个<strong>实际值</strong>，而不是分类结果，我们需要某个<strong>函数</strong>来建立输入与输出之间的映射</p>
<p>例，市场价格预测，降水量预测等问题</p>
<p>分类问题，利用<strong>离散</strong>的输出数据来预测结果，因此，通过输入变量预测出这一样本所属的<strong>类别</strong>，以<strong>不同的类别</strong>来建立输入与输出之间的映射</p>
<p>例，植物品种、客户年龄和偏好的预测问题</p>
</blockquote>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><blockquote>
<p>得到一个数据集，<strong>不知道</strong>其正确的结果（输出数据）/标签/属性，或者说<strong>所有的数据都是一样的</strong>（没有区别），但可以找到数据中<strong>存在某种结构</strong>（不确定的），且不用关注变量（输入数据）的影响</p>
<p>通过研究<strong>变量之间的联系</strong>来推测出该结构</p>
<p>没有基于预测结果的反馈</p>
<p>例，设想我们有一批照片其中包含着不同颜色的几何形状。在这里计算机面对的是没有任何标记的图片，它并不知道几何形状的颜色和外形，它看到的只是一张张照片而已。但通过将数据输入到非监督学习的模型中去，算法可以尝试着理解图中的内容，通过相关性和特征将图中的相似的物体聚为一类。在<strong>理想的情况</strong>下它可以将不同形状不同颜色的几何形状聚集到不同的类别中去，特征提取和标签都是<strong>机器自己完成</strong>的。</p>
</blockquote>
<p>使用场景</p>
<ul>
<li>发现异常，通过无监督学习，快速将数据分类，虽然不知道这些分类的意义，但可以快速排出正常的数据，更有针对性的对异常数据进行深入分析</li>
<li>用户细分，如广告平台对用户按性别，年龄，位置，行为等维度进行用户细分，使广告投放更有针对性</li>
<li>推荐系统，如淘宝根据我们的浏览行为推荐相关商品。无监督学习通过聚类得到购物行为相似的用户，然后将这类用户的商品推荐给我们</li>
</ul>
<blockquote>
<p>Clustering（聚类算法），自动分类的方法，但不知道聚类后的几个分类分别代表什么</p>
<p>例如收集1,000,000个不同的基因，并找到一种方法，如不同变量（例如寿命，位置，角色等）之间的联系，将这些基因自动分组为在某种程度上相似或相关的组，即<strong>存在的某种结构</strong>。</p>
<p>我们只告知这个方法，有一堆（未知）数据，然后自动找到这些数据中的类型，并根据所找到类型对数据进行分类。也就是说，对于数据样本，我们没有给方法一个正确的答案，这便是无监督学习。</p>
<p>Non-clustering</p>
<p>降维</p>
</blockquote>
<h2 id="二者对比"><a href="#二者对比" class="headerlink" title="二者对比"></a>二者对比</h2><div class="table-container">
<table>
<thead>
<tr>
<th>监督学习</th>
<th>无监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>目的明确</strong>的训练方式，知道会得到什么结果</td>
<td><strong>没有明确</strong>的训练方式，无法提前知道结果是什么</td>
</tr>
<tr>
<td>数据<strong>有标签</strong></td>
<td>数据<strong>没有标签</strong></td>
</tr>
<tr>
<td>目标明确，可以衡量效果</td>
<td>几乎无法评估效果</td>
</tr>
<tr>
<td>一种训练方式/学习方式，有明确的目标，很清楚自己想要什么结果</td>
<td>本质上是一个统计手段，在没有标签的数据里发现潜在的结构</td>
</tr>
</tbody>
</table>
</div>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><blockquote>
<p>To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a <strong>hypothesis</strong>. </p>
</blockquote>
<p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306142126309.png" srcset="/img/loading.gif" lazyload style="zoom:80%;"></p>
<h1 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h1><blockquote>
<p>通过 cost Function 来确定我们的hypothesis，即，h(X)。</p>
<p>m表示样本数量</p>
</blockquote>
<script type="math/tex; mode=display">
h_\Theta(x)=\Theta_0+\Theta_1x</script><script type="math/tex; mode=display">
J(\Theta_0,\Theta_1)=\frac {1}{2m}\sum_{i=1}^m(\hat y_i-y_i)^2=\frac {1}{2m}\sum_{i=1}^m(h_\Theta(x_i)-y_i)^2</script><blockquote>
<p>cost Function 越小，表示hypothesis（拟合函数）约合理</p>
<p>因此寻找出使 cost Function 最小的参数</p>
</blockquote>
<h1 id="梯度下降算法Gradient-Descent"><a href="#梯度下降算法Gradient-Descent" class="headerlink" title="梯度下降算法Gradient Descent"></a>梯度下降算法Gradient Descent</h1><p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306163104402.png" srcset="/img/loading.gif" lazyload alt></p>
<p>重复，直到趋近最小值(收敛)</p>
<script type="math/tex; mode=display">
\Theta_j:=\Theta_j-\alpha\frac {\partial}{\partial{\Theta_j}}J(\Theta_0,\Theta_1)</script><blockquote>
<p>首先假设一对$\Theta_0,\Theta_1$,然后由costFunction的导数确定梯度下降最大的方向，逐步找到costFunction的最小值（可能是局部最小值）</p>
<p>参数$\alpha$(学习速率)确定每一步的大小，即更新$\Theta_j$大小的幅度</p>
<p>不同的起点，可能会导致不同的结果，这也是梯度下降算法的重要特征</p>
</blockquote>
<p>梯度下降方程，需同步更新$\Theta_0,\Theta_1$</p>
<p>$temp0:=\Theta_0-\alpha\frac {\partial}{\partial{\Theta_0}}J(\Theta_0,\Theta_1)$</p>
<p>$temp1:=\Theta_1-\alpha\frac {\partial}{\partial{\Theta_1}}J(\Theta_0,\Theta_1)$</p>
<p>$\Theta_0:=temp0$</p>
<p>$\Theta_1:=temp1$</p>
<blockquote>
<p>利用梯度下降法，可以优化找到<strong>任何</strong>costFunction的(局部)最小值</p>
</blockquote>
<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><script type="math/tex; mode=display">
h_\Theta(x)=\Theta_0+\Theta_1x</script><p>线性回归不仅适用于2个变量（二维平面），也适用于多个变量。</p>
<p>如，3个变量是一个平面，4个变量是一个体</p>
<p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210307131430617.png" srcset="/img/loading.gif" lazyload alt></p>
<blockquote>
<p>梯度下降法——线性回归，得到的是最优解，而不是局部最优解</p>
</blockquote>
<p>重复，直到收敛</p>
<p>$temp0:=\Theta_0-\alpha\frac {1}{m}\sum_{i=1}^{m}(h_\Theta(x_i)-y_i)$</p>
<p>$temp1:=\Theta_1-\alpha\frac {1}{m}\sum_{i=1}^{m}((h_\Theta(x_i)-y_i)x_i)$</p>
<p>$\Theta_0:=temp0$</p>
<p>$\Theta_1:=temp1$</p>
<h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><blockquote>
<p>n表示特征变量个数</p>
</blockquote>
<p>hypothesis:</p>
<script type="math/tex; mode=display">
h_\Theta(x)=\Theta_0+\Theta_1x_1+\Theta_2x_2+\cdots+\Theta_nx_n</script><p>由矩阵乘法，简化上式</p>
<script type="math/tex; mode=display">
h_\Theta(x)=
\begin{bmatrix}
\Theta_0&\Theta_1\cdots\Theta_n
\end{bmatrix}
\begin{bmatrix}
x_0\\\\
x_1\\\\
\vdots\\\\
x_n
\end{bmatrix}
=\Theta^Tx</script><h2 id="多个变量的梯度下降"><a href="#多个变量的梯度下降" class="headerlink" title="多个变量的梯度下降"></a>多个变量的梯度下降</h2><script type="math/tex; mode=display">
\Theta_j:=\Theta_j-\alpha\frac {1}{m}\sum_{i=1}^{m}(h_\Theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><h2 id="优化梯度下降算法"><a href="#优化梯度下降算法" class="headerlink" title="优化梯度下降算法"></a>优化梯度下降算法</h2><p>当样本不均匀时，如0&lt;$x_1$&lt;20000，0&lt;$x_2$&lt;5，$\Theta$下降会十分缓慢，因此采用<strong>特征放缩</strong>和<strong>均值归一化</strong>将所有变量大致控制在较小范围</p>
<script type="math/tex; mode=display">
x_i:=\frac{x_i-\mu_i}{s_i}</script><blockquote>
<p>$\mu_i$为平均值</p>
<p>$s_i$为标准差</p>
</blockquote>
<h2 id="学习率a"><a href="#学习率a" class="headerlink" title="学习率a"></a>学习率a</h2><blockquote>
<p>a太小，$J(\Theta)$收敛很慢</p>
<p>a太大，每一次迭代$J(\Theta)$可能不会减少，无法收敛</p>
</blockquote>
<h2 id="特征与多项式回归"><a href="#特征与多项式回归" class="headerlink" title="特征与多项式回归"></a>特征与多项式回归</h2><blockquote>
<p>运用不同的方法改善<strong>特征变量</strong>和<strong>hypothesis</strong></p>
<p>如由$x_1*x_2$创建新的特征变量$x_3$</p>
</blockquote>
<p>当线性函数无法满足数据要求，更改<strong>hypothesis</strong>函数，如二次，三次等形式。</p>
<p>例如，</p>
<p>原hypothesis函数</p>
<script type="math/tex; mode=display">
h(\Theta)=\Theta_1+\Theta_2x_1</script><p>改进为</p>
<script type="math/tex; mode=display">
h(\Theta)=\Theta_1+\Theta_2x_1+\Theta_3x_1^2</script><p>更改新特征变量的范围</p>
<blockquote>
<p>如$0&lt;x_1&lt;10$，则$0&lt;x_1^2&lt;100$</p>
</blockquote>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>一次性求解$\theta$的最优值，不需要像梯度下降法多次迭代</p>
<script type="math/tex; mode=display">
\theta=(X^TX)^{-1}X^Ty</script><p>有m个样本$(x^{(1)},y^{(1)},…,(x^{(m)},y^{(m)}))$，n个特征变量</p>
<script type="math/tex; mode=display">
x^{(i)}=
\begin{bmatrix}
x_0^{(i)}\\\\
x_1^{(i)}\\\\
\vdots\\\\
x_n^{(i)}\\\\
\end{bmatrix}
\in R^{n+1}</script><p>设计矩阵design matrix (m*(n+1))</p>
<script type="math/tex; mode=display">
X=\begin{bmatrix}
(x^{(1)})^T\\\\
(x^{(2)})^T\\\\
\vdots\\\\
(x^{(m)})^T
\end{bmatrix}</script><div class="table-container">
<table>
<thead>
<tr>
<th>正规方程</th>
<th>梯度下降法</th>
</tr>
</thead>
<tbody>
<tr>
<td>不需要学习率a</td>
<td>需要学习率a</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>需要多次迭代</td>
</tr>
<tr>
<td>$O(n^3)$，需要计算$X^TX$</td>
<td>$O(kn^2)$</td>
</tr>
<tr>
<td>n很大时，运算很慢</td>
<td>适合于n很大(n&gt;10000)</td>
</tr>
</tbody>
</table>
</div>
<p>当$X^TX$不可逆，</p>
<p>原因</p>
<blockquote>
<ul>
<li>存在两个特征变量，它们（近似）线性相关</li>
<li>m&lt;n，样本数量少于特征变量数量</li>
</ul>
</blockquote>
<p>解决</p>
<blockquote>
<ul>
<li>删去存在线性相关的特征变量</li>
<li>减少特征变量</li>
</ul>
</blockquote>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑函数Logistic/Sigmoid Function</p>
<script type="math/tex; mode=display">
h_\theta(x)=g(\theta^Tx)
\\\\
z=\theta^Tx
\\\\
g(z)=\frac {1}{1+e^{-z}}</script><p>图像$g(z)$</p>
<p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316161356522.png" srcset="/img/loading.gif" lazyload alt></p>
<script type="math/tex; mode=display">
h_\theta(x)=P(y=1|x;\theta)=1-P(y=0|x;\theta)</script><h2 id="决策边界Decision-Boundary"><a href="#决策边界Decision-Boundary" class="headerlink" title="决策边界Decision Boundary"></a>决策边界Decision Boundary</h2><p>假设</p>
<script type="math/tex; mode=display">
h_\theta(x)\ge0.5\rightarrow y=1\\
h_\theta(x)\le0.5\rightarrow y=0</script><p>因此</p>
<script type="math/tex; mode=display">
\theta^Tx\ge0\Rightarrow y=1\\
\theta^Tx\le0\Rightarrow y=0</script><p><strong>decision boundary</strong>可以是任意图像，将区域划分为两个部分，$y=1$和$y=0$</p>
<p>通过给定的$\theta$来找到<strong>decision boundary</strong></p>
<p>通过<strong>hypothesis function</strong>来找到合适的$\theta$</p>
<h2 id="Cost-Function-1"><a href="#Cost-Function-1" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>训练集，m个样本</p>
<script type="math/tex; mode=display">
\begin{Bmatrix}
(x^{(1)},y^{(1)},x^{(2)},y^{(2)},\cdots,x^{(m)},y^{(m)})
\end{Bmatrix}
\\
x=\in 
\begin{bmatrix}
x_0\\\\
x_1\\\\
\vdots\\\\
x_n
\end{bmatrix}
\ \ \ \ 
x_0=1\ \ \ \ 
y=
\begin{Bmatrix}
0,1
\end{Bmatrix}</script><p><strong>hypothesis function</strong></p>
<script type="math/tex; mode=display">
h_\theta(x)=\frac {1}{1+e^{-\theta^Tx}}</script><p><strong>Logistic regression cost function</strong></p>
<script type="math/tex; mode=display">
Cost(h_\theta(x),y)=
\begin{cases}
-log(h_\theta(x)),& if \ y=1
\\\\
-log(1-h_\theta(x)),&if\ y=0
\end{cases}</script><p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316172644451.png" srcset="/img/loading.gif" lazyload style="zoom:50%;"><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316172711040.png" srcset="/img/loading.gif" lazyload style="zoom:50%;"></p>
<script type="math/tex; mode=display">
Cost(h_\theta(x),y)=0 \ if \ h_\theta(x)=y\\
Cost(h_\theta(x),y)\rightarrow\infty \ if \ y=0\ and\ h_\theta(x)\rightarrow1\\
Cost(h_\theta(x),y)\rightarrow\infty \ if \ y=1 \ and\ h_\theta(x)\rightarrow0</script><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}[\sum_{i=1}^mCost(h_\theta(x),y)]\\\\
=-\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})(1-logh_\theta(x^{(i)}))]</script><p>向量化</p>
<script type="math/tex; mode=display">
h=g(X\theta)\\\\
J(\theta)=\frac{1}{m}(-y^Tlog(h)-(1-y)^Tlog(1-h))</script><p>得到$min_\theta J(\theta)$，多次迭代</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha\frac {\partial}{\partial{\theta_j}}J(\theta)\\\\
\theta_j:=\theta_j-\alpha\frac {1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>向量化</p>
<script type="math/tex; mode=display">
\theta:=\theta-\frac{\alpha}{m}X^T(g(X\theta)-\overrightarrow{y})</script><h1 id="过度拟合-Overfitting"><a href="#过度拟合-Overfitting" class="headerlink" title="过度拟合 Overfitting"></a>过度拟合 Overfitting</h1><p>由于设置过多特征变量，<strong>hypothesis</strong>几乎适应所有的训练样本，但该<strong>hypothesis</strong>并不能进行很好的预测。</p>
<p>因为这个复杂的函数创建了许多与数据没有关联的曲线</p>
<p>解决方法</p>
<blockquote>
<p>减少特征变量</p>
<p>正规化Regularization</p>
</blockquote>
<h2 id="Regularization-Cost-Function"><a href="#Regularization-Cost-Function" class="headerlink" title="Regularization Cost Function"></a>Regularization Cost Function</h2><script type="math/tex; mode=display">
min_\theta\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2</script><h2 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h2><p><strong>Gradient Descent</strong></p>
<p>多次迭代</p>
<script type="math/tex; mode=display">
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\\\
\theta_j:=\theta_j-\alpha[(\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)})+\frac{\lambda}{m}\theta_j]</script><p><strong>正规方程</strong></p>
<script type="math/tex; mode=display">
\theta=(X^TX+\lambda L)^{-1}X^Ty\\\\
L=
\begin{bmatrix}
0&0&\cdots&0\\\\
0&1&\cdots&0\\\\
\vdots&\vdots&\ddots&\vdots\\\\
0&0&\cdots&1
\end{bmatrix}
\in R^{n+1}</script><h2 id="Regularized-Logistic-Regression"><a href="#Regularized-Logistic-Regression" class="headerlink" title="Regularized Logistic Regression"></a>Regularized Logistic Regression</h2><script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})(1-logh_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><h1 id="神经网络-Neural-Network"><a href="#神经网络-Neural-Network" class="headerlink" title="神经网络 Neural Network"></a>神经网络 Neural Network</h1><p>In our model, our dendrites are like the input features $x_1\cdots x_n$, and the output is the result of our hypothesis function. </p>
<p>In this model our $x_0$ input node is sometimes called the “bias unit.” It is always equal to 1.</p>
<p>In neural networks, we use the same logistic function as in classification, $\frac{1}{1 + e^{-\theta^Tx}}$, yet we sometimes call it a sigmoid (logistic) <strong>activation</strong> function. In this situation, our “theta” parameters are sometimes called “weights”.</p>
<p>Our input nodes (layer 1), also known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.</p>
<p>We can have <strong>intermediate layers</strong> of nodes between the input and output layers called the <strong>“hidden layers.”</strong></p>
<p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210324100709079.png" srcset="/img/loading.gif" lazyload alt></p>
<p>This is saying that we compute our activation nodes by using a 3×4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix$ \Theta^{(2)} $containing the weights for our second layer of nodes.</p>
<p>Each layer gets its own matrix of weights, $\Theta^{(j)}$.</p>
<p>The dimensions of these matrices of weights is determined as follows:</p>
<p>If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$.</p>
<p>The +1 comes from the addition in$ \Theta^{(j)}$ of the “bias nodes,”$ x_0 $and$ \Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will. The following image summarizes our model representation: </p>
<p><img src="/91a709bb/image-20210324101135246.png" srcset="/img/loading.gif" lazyload alt></p>
<p>Example: If layer 1 has 2 input nodes and layer 2 has 4 activation nodes. Dimension of $\Theta^{(1)}$ is going to be 4×3 where $s_j$ =2 and $s_{j+1} = 4$, so $s_{j+1} \times (s_j + 1) = 4 \times 3$</p>
<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>an example of a neural network</p>
<script type="math/tex; mode=display">
\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}</script><p>define a new variable $z_k^{(j)}$that encompasses the parameters inside our g function.</p>
<script type="math/tex; mode=display">
\begin{align*}a_1^{(2)} = g(z_1^{(2)}) \newline a_2^{(2)} = g(z_2^{(2)}) \newline a_3^{(2)} = g(z_3^{(2)}) \newline \end{align*}</script><p>In other words, for layer j=2 and node k, the variable z will be:</p>
<p>The vector representation of x and $z^{j}$ is:</p>
<script type="math/tex; mode=display">
\begin{align*}x = \begin{bmatrix}x_0 \newline x_1 \newline\cdots \newline x_n\end{bmatrix} &z^{(j)} = \begin{bmatrix}z_1^{(j)} \newline z_2^{(j)} \newline\cdots \newline z_n^{(j)}\end{bmatrix}\end{align*}</script><p>Setting $x = a^{(1)}$, we can rewrite the equation as:</p>
<p>We are multiplying our matrix $\Theta^{(j-1)}$ with dimensions $s_j\times (n+1)$(where $s_j$ is the number of our activation nodes) by our vector $a^{(j-1)}$ with height (n+1). This gives us our vector $z^{(j)}$ with height $s_j$. Now we can get a vector of our activation nodes for layer j as follows:</p>
<p>$a^{(j)} = g(z^{(j)})$</p>
<p>Where our function g can be applied element-wise to our vector $z^{(j)}$.</p>
<p>We can then add a bias unit (equal to 1) to layer j after we have computed $a^{(j)}$. This will be element $a_0^{(j)}$ and will be equal to 1. To compute our final hypothesis, let’s first compute another z vector:</p>
<p>$z^{(j+1)} = \Theta^{(j)}a^{(j)}$</p>
<p>We get this final z vector by multiplying the next theta matrix after $\Theta^{(j-1)}$with the values of all the activation nodes we just got. This last theta matrix $\Theta^{(j)}$ will have only <strong>one row</strong> which is multiplied by one column $a^{(j)}$ so that our result is a single number. We then get our final result with:</p>
<p>$h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)})$</p>
<p>Notice that in this <strong>last step</strong>, between layer j and layer j+1, we are doing <strong>exactly the same thing</strong> as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses.</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/72510051/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">监督学习与无监督学习</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/c2b6c13e/">
                        <span class="hidden-mobile">Java-异常</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div style="font-size: 0.85rem"> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/vvd_js/duration.js"></script> </div> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
