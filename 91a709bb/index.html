<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="google0d797e37a75de008.html">
  <meta name="msvalidate.01" content="F9FEEFCDA94FD78D74D61C9BAD778567" />
  <meta name="baidu-site-verification" content="code-cF8YWrbY30" />

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@latest/pace-theme-default.min.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@latest/pace.min.js"></script>

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"mihaoyoung.github.io","root":"/","scheme":"Pisces","version":"8.0.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="学习《吴恩达机器学习》的笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达——机器学习">
<meta property="og:url" content="https://mihaoyoung.github.io/91a709bb/">
<meta property="og:site_name" content="养羊小站">
<meta property="og:description" content="学习《吴恩达机器学习》的笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306142126309.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306163104402.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210307131430617.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316161356522.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316172644451.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316172711040.png">
<meta property="og:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210324100709079.png">
<meta property="og:image" content="https://mihaoyoung.github.io/91a709bb/image-20210324101135246.png">
<meta property="article:published_time" content="2021-03-05T14:16:50.000Z">
<meta property="article:modified_time" content="2021-03-24T02:43:41.120Z">
<meta property="article:author" content="Mihao">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306142126309.png">


<link rel="canonical" href="https://mihaoyoung.github.io/91a709bb/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>吴恩达——机器学习 | 养羊小站</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">养羊小站</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">一个小小博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-guestbook">

    <a href="/guestbook" rel="section"><i class="fas fa-book fa-fw"></i>留言板</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.1.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">无监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E8%80%85%E5%AF%B9%E6%AF%94"><span class="nav-number">1.3.</span> <span class="nav-text">二者对比</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Cost-Function"><span class="nav-number">3.</span> <span class="nav-text">Cost Function</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95Gradient-Descent"><span class="nav-number">4.</span> <span class="nav-text">梯度下降算法Gradient Descent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">5.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">5.1.</span> <span class="nav-text">多元线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E4%B8%AA%E5%8F%98%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">5.2.</span> <span class="nav-text">多个变量的梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="nav-number">5.3.</span> <span class="nav-text">优化梯度下降算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87a"><span class="nav-number">5.4.</span> <span class="nav-text">学习率a</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E4%B8%8E%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-number">5.5.</span> <span class="nav-text">特征与多项式回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="nav-number">5.6.</span> <span class="nav-text">正规方程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">6.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8CDecision-Boundary"><span class="nav-number">6.1.</span> <span class="nav-text">决策边界Decision Boundary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost-Function-1"><span class="nav-number">6.2.</span> <span class="nav-text">Cost Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Descent"><span class="nav-number">6.3.</span> <span class="nav-text">Gradient Descent</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88-Overfitting"><span class="nav-number">7.</span> <span class="nav-text">过度拟合 Overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularization-Cost-Function"><span class="nav-number">7.1.</span> <span class="nav-text">Regularization Cost Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularized-Linear-Regression"><span class="nav-number">7.2.</span> <span class="nav-text">Regularized Linear Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularized-Logistic-Regression"><span class="nav-number">7.3.</span> <span class="nav-text">Regularized Logistic Regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Neural-Network"><span class="nav-number">8.</span> <span class="nav-text">神经网络 Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96"><span class="nav-number">8.1.</span> <span class="nav-text">向量化</span></a></li></ol></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Mihao"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">Mihao</p>
  <div class="site-description" itemprop="description">努力写出负BUG</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/MihaoYoung" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;MihaoYoung" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:haominyoung@gmail.com" title="E-Mail → mailto:haominyoung@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1794589604@qq.com" title="QQ-Mail → mailto:1794589604@qq.com" rel="noopener" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll animated">
    <div class="links-of-blogroll-title"><i class="fas fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://cn.bing.com/" title="https:&#x2F;&#x2F;cn.bing.com&#x2F;" rel="noopener" target="_blank">Bing</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com&#x2F;" rel="noopener" target="_blank">Google</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.bilibili.com/" title="https:&#x2F;&#x2F;www.bilibili.com&#x2F;" rel="noopener" target="_blank">BiliBili</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.youtube.com/" title="https:&#x2F;&#x2F;www.youtube.com&#x2F;" rel="noopener" target="_blank">YouTube</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;" rel="noopener" target="_blank">知乎</a>
        </li>
    </ul>
  </div>

      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://mihaoyoung.github.io/91a709bb/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="Mihao">
      <meta itemprop="description" content="努力写出负BUG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="养羊小站">
    </span>

    
    
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达——机器学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-03-05 22:16:50" itemprop="dateCreated datePublished" datetime="2021-03-05T22:16:50+08:00">2021-03-05</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-03-24 10:43:41" itemprop="dateModified" datetime="2021-03-24T10:43:41+08:00">2021-03-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span id="/91a709bb/" class="post-meta-item leancloud_visitors" data-flag-title="吴恩达——机器学习" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

            <div class="post-description">学习《吴恩达机器学习》的笔记</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.                                                                ——Tom Mitchell</p>
</blockquote>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Example: playing checkers.</span><br><span class="line"></span><br><span class="line">E = the experience of playing many games of checkers</span><br><span class="line"></span><br><span class="line">T = the task of playing checkers.</span><br><span class="line"></span><br><span class="line">P = the probability that the program will win the next game.</span><br></pre></td></tr></table></figure>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><blockquote>
<p>得到一个数据集，知道<strong>正确</strong>的输出数据，并且认为输入和输出数据之间<strong>存在某种关系</strong>，</p>
<p>即，<strong>在知道输入和输出的情况下训练出一个模型，将输入映射到输出</strong></p>
</blockquote>
<p>流程</p>
<ul>
<li>选择一个适合目标任务的数学模型</li>
<li>先把一部分已知的“问题和答案”（训练集）给机器去学习</li>
<li>机器总结出了自己的“方法论”</li>
<li>人类把”新的问题”（测试集）给机器，让他去解答</li>
</ul>
<p>监督学习分为两大问题，”<strong>regression</strong>“和”<strong>classification</strong>“</p>
<blockquote>
<p>回归问题，利用<strong>连续</strong>的输出数据来预测结果，得到的是一个<strong>实际值</strong>，而不是分类结果，我们需要某个<strong>函数</strong>来建立输入与输出之间的映射</p>
<p>例，市场价格预测，降水量预测等问题</p>
<p>分类问题，利用<strong>离散</strong>的输出数据来预测结果，因此，通过输入变量预测出这一样本所属的<strong>类别</strong>，以<strong>不同的类别</strong>来建立输入与输出之间的映射</p>
<p>例，植物品种、客户年龄和偏好的预测问题</p>
</blockquote>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><blockquote>
<p>得到一个数据集，<strong>不知道</strong>其正确的结果（输出数据）/标签/属性，或者说<strong>所有的数据都是一样的</strong>（没有区别），但可以找到数据中<strong>存在某种结构</strong>（不确定的），且不用关注变量（输入数据）的影响</p>
<p>通过研究<strong>变量之间的联系</strong>来推测出该结构</p>
<p>没有基于预测结果的反馈</p>
<p>例，设想我们有一批照片其中包含着不同颜色的几何形状。在这里计算机面对的是没有任何标记的图片，它并不知道几何形状的颜色和外形，它看到的只是一张张照片而已。但通过将数据输入到非监督学习的模型中去，算法可以尝试着理解图中的内容，通过相关性和特征将图中的相似的物体聚为一类。在<strong>理想的情况</strong>下它可以将不同形状不同颜色的几何形状聚集到不同的类别中去，特征提取和标签都是<strong>机器自己完成</strong>的。</p>
</blockquote>
<p>使用场景</p>
<ul>
<li>发现异常，通过无监督学习，快速将数据分类，虽然不知道这些分类的意义，但可以快速排出正常的数据，更有针对性的对异常数据进行深入分析</li>
<li>用户细分，如广告平台对用户按性别，年龄，位置，行为等维度进行用户细分，使广告投放更有针对性</li>
<li>推荐系统，如淘宝根据我们的浏览行为推荐相关商品。无监督学习通过聚类得到购物行为相似的用户，然后将这类用户的商品推荐给我们</li>
</ul>
<blockquote>
<p>Clustering（聚类算法），自动分类的方法，但不知道聚类后的几个分类分别代表什么</p>
<p>例如收集1,000,000个不同的基因，并找到一种方法，如不同变量（例如寿命，位置，角色等）之间的联系，将这些基因自动分组为在某种程度上相似或相关的组，即<strong>存在的某种结构</strong>。</p>
<p>我们只告知这个方法，有一堆（未知）数据，然后自动找到这些数据中的类型，并根据所找到类型对数据进行分类。也就是说，对于数据样本，我们没有给方法一个正确的答案，这便是无监督学习。</p>
<p>Non-clustering</p>
<p>降维</p>
</blockquote>
<h2 id="二者对比"><a href="#二者对比" class="headerlink" title="二者对比"></a>二者对比</h2><div class="table-container">
<table>
<thead>
<tr>
<th>监督学习</th>
<th>无监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>目的明确</strong>的训练方式，知道会得到什么结果</td>
<td><strong>没有明确</strong>的训练方式，无法提前知道结果是什么</td>
</tr>
<tr>
<td>数据<strong>有标签</strong></td>
<td>数据<strong>没有标签</strong></td>
</tr>
<tr>
<td>目标明确，可以衡量效果</td>
<td>几乎无法评估效果</td>
</tr>
<tr>
<td>一种训练方式/学习方式，有明确的目标，很清楚自己想要什么结果</td>
<td>本质上是一个统计手段，在没有标签的数据里发现潜在的结构</td>
</tr>
</tbody>
</table>
</div>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><blockquote>
<p>To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a <strong>hypothesis</strong>. </p>
</blockquote>
<p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306142126309.png" style="zoom:80%;"></p>
<h1 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h1><blockquote>
<p>通过 cost Function 来确定我们的hypothesis，即，h(X)。</p>
<p>m表示样本数量</p>
</blockquote>
<script type="math/tex; mode=display">
h_\Theta(x)=\Theta_0+\Theta_1x</script><script type="math/tex; mode=display">
J(\Theta_0,\Theta_1)=\frac {1}{2m}\sum_{i=1}^m(\hat y_i-y_i)^2=\frac {1}{2m}\sum_{i=1}^m(h_\Theta(x_i)-y_i)^2</script><blockquote>
<p>cost Function 越小，表示hypothesis（拟合函数）约合理</p>
<p>因此寻找出使 cost Function 最小的参数</p>
</blockquote>
<h1 id="梯度下降算法Gradient-Descent"><a href="#梯度下降算法Gradient-Descent" class="headerlink" title="梯度下降算法Gradient Descent"></a>梯度下降算法Gradient Descent</h1><p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210306163104402.png" alt></p>
<p>重复，直到趋近最小值(收敛)</p>
<script type="math/tex; mode=display">
\Theta_j:=\Theta_j-\alpha\frac {\partial}{\partial{\Theta_j}}J(\Theta_0,\Theta_1)</script><blockquote>
<p>首先假设一对$\Theta_0,\Theta_1$,然后由costFunction的导数确定梯度下降最大的方向，逐步找到costFunction的最小值（可能是局部最小值）</p>
<p>参数$\alpha$(学习速率)确定每一步的大小，即更新$\Theta_j$大小的幅度</p>
<p>不同的起点，可能会导致不同的结果，这也是梯度下降算法的重要特征</p>
</blockquote>
<p>梯度下降方程，需同步更新$\Theta_0,\Theta_1$</p>
<p>$temp0:=\Theta_0-\alpha\frac {\partial}{\partial{\Theta_0}}J(\Theta_0,\Theta_1)$</p>
<p>$temp1:=\Theta_1-\alpha\frac {\partial}{\partial{\Theta_1}}J(\Theta_0,\Theta_1)$</p>
<p>$\Theta_0:=temp0$</p>
<p>$\Theta_1:=temp1$</p>
<blockquote>
<p>利用梯度下降法，可以优化找到<strong>任何</strong>costFunction的(局部)最小值</p>
</blockquote>
<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><script type="math/tex; mode=display">
h_\Theta(x)=\Theta_0+\Theta_1x</script><p>线性回归不仅适用于2个变量（二维平面），也适用于多个变量。</p>
<p>如，3个变量是一个平面，4个变量是一个体</p>
<p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210307131430617.png" alt></p>
<blockquote>
<p>梯度下降法——线性回归，得到的是最优解，而不是局部最优解</p>
</blockquote>
<p>重复，直到收敛</p>
<p>$temp0:=\Theta_0-\alpha\frac {1}{m}\sum_{i=1}^{m}(h_\Theta(x_i)-y_i)$</p>
<p>$temp1:=\Theta_1-\alpha\frac {1}{m}\sum_{i=1}^{m}((h_\Theta(x_i)-y_i)x_i)$</p>
<p>$\Theta_0:=temp0$</p>
<p>$\Theta_1:=temp1$</p>
<h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><blockquote>
<p>n表示特征变量个数</p>
</blockquote>
<p>hypothesis:</p>
<script type="math/tex; mode=display">
h_\Theta(x)=\Theta_0+\Theta_1x_1+\Theta_2x_2+\cdots+\Theta_nx_n</script><p>由矩阵乘法，简化上式</p>
<script type="math/tex; mode=display">
h_\Theta(x)=
\begin{bmatrix}
\Theta_0&\Theta_1\cdots\Theta_n
\end{bmatrix}
\begin{bmatrix}
x_0\\\\
x_1\\\\
\vdots\\\\
x_n
\end{bmatrix}
=\Theta^Tx</script><h2 id="多个变量的梯度下降"><a href="#多个变量的梯度下降" class="headerlink" title="多个变量的梯度下降"></a>多个变量的梯度下降</h2><script type="math/tex; mode=display">
\Theta_j:=\Theta_j-\alpha\frac {1}{m}\sum_{i=1}^{m}(h_\Theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><h2 id="优化梯度下降算法"><a href="#优化梯度下降算法" class="headerlink" title="优化梯度下降算法"></a>优化梯度下降算法</h2><p>当样本不均匀时，如0&lt;$x_1$&lt;20000，0&lt;$x_2$&lt;5，$\Theta$下降会十分缓慢，因此采用<strong>特征放缩</strong>和<strong>均值归一化</strong>将所有变量大致控制在较小范围</p>
<script type="math/tex; mode=display">
x_i:=\frac{x_i-\mu_i}{s_i}</script><blockquote>
<p>$\mu_i$为平均值</p>
<p>$s_i$为标准差</p>
</blockquote>
<h2 id="学习率a"><a href="#学习率a" class="headerlink" title="学习率a"></a>学习率a</h2><blockquote>
<p>a太小，$J(\Theta)$收敛很慢</p>
<p>a太大，每一次迭代$J(\Theta)$可能不会减少，无法收敛</p>
</blockquote>
<h2 id="特征与多项式回归"><a href="#特征与多项式回归" class="headerlink" title="特征与多项式回归"></a>特征与多项式回归</h2><blockquote>
<p>运用不同的方法改善<strong>特征变量</strong>和<strong>hypothesis</strong></p>
<p>如由$x_1*x_2$创建新的特征变量$x_3$</p>
</blockquote>
<p>当线性函数无法满足数据要求，更改<strong>hypothesis</strong>函数，如二次，三次等形式。</p>
<p>例如，</p>
<p>原hypothesis函数</p>
<script type="math/tex; mode=display">
h(\Theta)=\Theta_1+\Theta_2x_1</script><p>改进为</p>
<script type="math/tex; mode=display">
h(\Theta)=\Theta_1+\Theta_2x_1+\Theta_3x_1^2</script><p>更改新特征变量的范围</p>
<blockquote>
<p>如$0&lt;x_1&lt;10$，则$0&lt;x_1^2&lt;100$</p>
</blockquote>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>一次性求解$\theta$的最优值，不需要像梯度下降法多次迭代</p>
<script type="math/tex; mode=display">
\theta=(X^TX)^{-1}X^Ty</script><p>有m个样本$(x^{(1)},y^{(1)},…,(x^{(m)},y^{(m)}))$，n个特征变量</p>
<script type="math/tex; mode=display">
x^{(i)}=
\begin{bmatrix}
x_0^{(i)}\\\\
x_1^{(i)}\\\\
\vdots\\\\
x_n^{(i)}\\\\
\end{bmatrix}
\in R^{n+1}</script><p>设计矩阵design matrix (m*(n+1))</p>
<script type="math/tex; mode=display">
X=\begin{bmatrix}
(x^{(1)})^T\\\\
(x^{(2)})^T\\\\
\vdots\\\\
(x^{(m)})^T
\end{bmatrix}</script><div class="table-container">
<table>
<thead>
<tr>
<th>正规方程</th>
<th>梯度下降法</th>
</tr>
</thead>
<tbody>
<tr>
<td>不需要学习率a</td>
<td>需要学习率a</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>需要多次迭代</td>
</tr>
<tr>
<td>$O(n^3)$，需要计算$X^TX$</td>
<td>$O(kn^2)$</td>
</tr>
<tr>
<td>n很大时，运算很慢</td>
<td>适合于n很大(n&gt;10000)</td>
</tr>
</tbody>
</table>
</div>
<p>当$X^TX$不可逆，</p>
<p>原因</p>
<blockquote>
<ul>
<li>存在两个特征变量，它们（近似）线性相关</li>
<li>m&lt;n，样本数量少于特征变量数量</li>
</ul>
</blockquote>
<p>解决</p>
<blockquote>
<ul>
<li>删去存在线性相关的特征变量</li>
<li>减少特征变量</li>
</ul>
</blockquote>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑函数Logistic/Sigmoid Function</p>
<script type="math/tex; mode=display">
h_\theta(x)=g(\theta^Tx)
\\\\
z=\theta^Tx
\\\\
g(z)=\frac {1}{1+e^{-z}}</script><p>图像$g(z)$</p>
<p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316161356522.png" alt></p>
<script type="math/tex; mode=display">
h_\theta(x)=P(y=1|x;\theta)=1-P(y=0|x;\theta)</script><h2 id="决策边界Decision-Boundary"><a href="#决策边界Decision-Boundary" class="headerlink" title="决策边界Decision Boundary"></a>决策边界Decision Boundary</h2><p>假设</p>
<script type="math/tex; mode=display">
h_\theta(x)\ge0.5\rightarrow y=1\\
h_\theta(x)\le0.5\rightarrow y=0</script><p>因此</p>
<script type="math/tex; mode=display">
\theta^Tx\ge0\Rightarrow y=1\\
\theta^Tx\le0\Rightarrow y=0</script><p><strong>decision boundary</strong>可以是任意图像，将区域划分为两个部分，$y=1$和$y=0$</p>
<p>通过给定的$\theta$来找到<strong>decision boundary</strong></p>
<p>通过<strong>hypothesis function</strong>来找到合适的$\theta$</p>
<h2 id="Cost-Function-1"><a href="#Cost-Function-1" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>训练集，m个样本</p>
<script type="math/tex; mode=display">
\begin{Bmatrix}
(x^{(1)},y^{(1)},x^{(2)},y^{(2)},\cdots,x^{(m)},y^{(m)})
\end{Bmatrix}
\\
x=\in 
\begin{bmatrix}
x_0\\\\
x_1\\\\
\vdots\\\\
x_n
\end{bmatrix}
\ \ \ \ 
x_0=1\ \ \ \ 
y=
\begin{Bmatrix}
0,1
\end{Bmatrix}</script><p><strong>hypothesis function</strong></p>
<script type="math/tex; mode=display">
h_\theta(x)=\frac {1}{1+e^{-\theta^Tx}}</script><p><strong>Logistic regression cost function</strong></p>
<script type="math/tex; mode=display">
Cost(h_\theta(x),y)=
\begin{cases}
-log(h_\theta(x)),& if \ y=1
\\\\
-log(1-h_\theta(x)),&if\ y=0
\end{cases}</script><p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316172644451.png" style="zoom:50%;"><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210316172711040.png" style="zoom:50%;"></p>
<script type="math/tex; mode=display">
Cost(h_\theta(x),y)=0 \ if \ h_\theta(x)=y\\
Cost(h_\theta(x),y)\rightarrow\infty \ if \ y=0\ and\ h_\theta(x)\rightarrow1\\
Cost(h_\theta(x),y)\rightarrow\infty \ if \ y=1 \ and\ h_\theta(x)\rightarrow0</script><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}[\sum_{i=1}^mCost(h_\theta(x),y)]\\\\
=-\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})(1-logh_\theta(x^{(i)}))]</script><p>向量化</p>
<script type="math/tex; mode=display">
h=g(X\theta)\\\\
J(\theta)=\frac{1}{m}(-y^Tlog(h)-(1-y)^Tlog(1-h))</script><p>得到$min_\theta J(\theta)$，多次迭代</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha\frac {\partial}{\partial{\theta_j}}J(\theta)\\\\
\theta_j:=\theta_j-\alpha\frac {1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>向量化</p>
<script type="math/tex; mode=display">
\theta:=\theta-\frac{\alpha}{m}X^T(g(X\theta)-\overrightarrow{y})</script><h1 id="过度拟合-Overfitting"><a href="#过度拟合-Overfitting" class="headerlink" title="过度拟合 Overfitting"></a>过度拟合 Overfitting</h1><p>由于设置过多特征变量，<strong>hypothesis</strong>几乎适应所有的训练样本，但该<strong>hypothesis</strong>并不能进行很好的预测。</p>
<p>因为这个复杂的函数创建了许多与数据没有关联的曲线</p>
<p>解决方法</p>
<blockquote>
<p>减少特征变量</p>
<p>正规化Regularization</p>
</blockquote>
<h2 id="Regularization-Cost-Function"><a href="#Regularization-Cost-Function" class="headerlink" title="Regularization Cost Function"></a>Regularization Cost Function</h2><script type="math/tex; mode=display">
min_\theta\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2</script><h2 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h2><p><strong>Gradient Descent</strong></p>
<p>多次迭代</p>
<script type="math/tex; mode=display">
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\\\
\theta_j:=\theta_j-\alpha[(\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)})+\frac{\lambda}{m}\theta_j]</script><p><strong>正规方程</strong></p>
<script type="math/tex; mode=display">
\theta=(X^TX+\lambda L)^{-1}X^Ty\\\\
L=
\begin{bmatrix}
0&0&\cdots&0\\\\
0&1&\cdots&0\\\\
\vdots&\vdots&\ddots&\vdots\\\\
0&0&\cdots&1
\end{bmatrix}
\in R^{n+1}</script><h2 id="Regularized-Logistic-Regression"><a href="#Regularized-Logistic-Regression" class="headerlink" title="Regularized Logistic Regression"></a>Regularized Logistic Regression</h2><script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})(1-logh_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><h1 id="神经网络-Neural-Network"><a href="#神经网络-Neural-Network" class="headerlink" title="神经网络 Neural Network"></a>神经网络 Neural Network</h1><p>In our model, our dendrites are like the input features $x_1\cdots x_n$, and the output is the result of our hypothesis function. </p>
<p>In this model our $x_0$ input node is sometimes called the “bias unit.” It is always equal to 1.</p>
<p>In neural networks, we use the same logistic function as in classification, $\frac{1}{1 + e^{-\theta^Tx}}$, yet we sometimes call it a sigmoid (logistic) <strong>activation</strong> function. In this situation, our “theta” parameters are sometimes called “weights”.</p>
<p>Our input nodes (layer 1), also known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.</p>
<p>We can have <strong>intermediate layers</strong> of nodes between the input and output layers called the <strong>“hidden layers.”</strong></p>
<p><img src="https://gitee.com/mihaoyoung/blog_image/raw/master/image-20210324100709079.png" alt></p>
<p>This is saying that we compute our activation nodes by using a 3×4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix$ \Theta^{(2)} $containing the weights for our second layer of nodes.</p>
<p>Each layer gets its own matrix of weights, $\Theta^{(j)}$.</p>
<p>The dimensions of these matrices of weights is determined as follows:</p>
<p>If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$.</p>
<p>The +1 comes from the addition in$ \Theta^{(j)}$ of the “bias nodes,”$ x_0 $and$ \Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will. The following image summarizes our model representation: </p>
<p><img src="/91a709bb/image-20210324101135246.png" alt></p>
<p>Example: If layer 1 has 2 input nodes and layer 2 has 4 activation nodes. Dimension of $\Theta^{(1)}$ is going to be 4×3 where $s_j$ =2 and $s_{j+1} = 4$, so $s_{j+1} \times (s_j + 1) = 4 \times 3$</p>
<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>an example of a neural network</p>
<script type="math/tex; mode=display">
\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}</script><p>define a new variable $z_k^{(j)}$that encompasses the parameters inside our g function.</p>
<script type="math/tex; mode=display">
\begin{align*}a_1^{(2)} = g(z_1^{(2)}) \newline a_2^{(2)} = g(z_2^{(2)}) \newline a_3^{(2)} = g(z_3^{(2)}) \newline \end{align*}</script><p>In other words, for layer j=2 and node k, the variable z will be:</p>
<p>The vector representation of x and $z^{j}$ is:</p>
<script type="math/tex; mode=display">
\begin{align*}x = \begin{bmatrix}x_0 \newline x_1 \newline\cdots \newline x_n\end{bmatrix} &z^{(j)} = \begin{bmatrix}z_1^{(j)} \newline z_2^{(j)} \newline\cdots \newline z_n^{(j)}\end{bmatrix}\end{align*}</script><p>Setting $x = a^{(1)}$, we can rewrite the equation as:</p>
<p>We are multiplying our matrix $\Theta^{(j-1)}$ with dimensions $s_j\times (n+1)$(where $s_j$ is the number of our activation nodes) by our vector $a^{(j-1)}$ with height (n+1). This gives us our vector $z^{(j)}$ with height $s_j$. Now we can get a vector of our activation nodes for layer j as follows:</p>
<p>$a^{(j)} = g(z^{(j)})$</p>
<p>Where our function g can be applied element-wise to our vector $z^{(j)}$.</p>
<p>We can then add a bias unit (equal to 1) to layer j after we have computed $a^{(j)}$. This will be element $a_0^{(j)}$ and will be equal to 1. To compute our final hypothesis, let’s first compute another z vector:</p>
<p>$z^{(j+1)} = \Theta^{(j)}a^{(j)}$</p>
<p>We get this final z vector by multiplying the next theta matrix after $\Theta^{(j-1)}$with the values of all the activation nodes we just got. This last theta matrix $\Theta^{(j)}$ will have only <strong>one row</strong> which is multiplied by one column $a^{(j)}$ so that our result is a single number. We then get our final result with:</p>
<p>$h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)})$</p>
<p>Notice that in this <strong>last step</strong>, between layer j and layer j+1, we are doing <strong>exactly the same thing</strong> as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses.</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Mihao
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://mihaoyoung.github.io/91a709bb/" title="吴恩达——机器学习">https://mihaoyoung.github.io/91a709bb/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fas fa-tags"></i> 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/c2b6c13e/" rel="prev" title="Java-异常">
                  <i class="fa fa-chevron-left"></i> Java-异常
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/72510051/" rel="next" title="监督学习与无监督学习">
                  监督学习与无监督学习 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
  
  
  



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mihao</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">44k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">40 分钟</span>
  </span>
</div><script color="242,129,214" opacity="0.8" zIndex="-1" count="120" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

<span>本站已运行<span id="showDays"></span></span>
<script>
  var seconds = 1000;
  var minutes = seconds * 60;
  var hours = minutes * 60;
  var days = hours * 24;
  var years = days * 365;
  var birthDay = Date.UTC(2021,02,20,08,00,00); // 这里设置建站时间
  setInterval(function() {
    var today = new Date();
    var todayYear = today.getFullYear();
    var todayMonth = today.getMonth()+1;
    var todayDate = today.getDate();
    var todayHour = today.getHours();
    var todayMinute = today.getMinutes();
    var todaySecond = today.getSeconds();
    var now = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
    var diff = now - birthDay;
    var diffYears = Math.floor(diff/years);
    var diffDays = Math.floor((diff/days)-diffYears*365);
    var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
    var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
    var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
      document.getElementById('showDays').innerHTML=""+diffYears+"年"+diffDays+"天"+diffHours+"小时"+diffMinutes+"分钟"+diffSeconds+"秒";
  }, 1000);
</script>


    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>












  








  

  
      <script>
  if (typeof MathJax === 'undefined') {
    // window.MathJax = {
    //  tex: {
    //    inlineMath: {'[+]': [['$', '$']]},
    //    tags: 'ams'
    //  },
    //  options: {
    //    renderActions: {
    //      findScript: [10, doc => {
    //        document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
    //          const display = !!node.type.match(/; *mode=display/);
    //          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
    //          const text = document.createTextNode('');
    //          node.parentNode.replaceChild(text, node);
    //          math.start = {node: text, delim: '', n: 0};
    //          math.end = {node: text, delim: '', n: 0};
    //         doc.math.push(math);
    //        });
    //      }, '', false],
    //      insertedScript: [200, () => {
    //        document.querySelectorAll('mjx-container').forEach(node => {
    //          const target = node.parentNode;
    //          if (target.nodeName.toLowerCase() === 'li') {
    //            target.parentNode.classList.add('has-jax');
    //          }
    //        });
    //     }, '', false]
    //   }
    //  }
    // };
	window.MathJax = {
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    };
    const script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
<script>
NexT.utils.loadComments('#valine-comments', () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', () => {
    new Valine(Object.assign({
      el  : '#valine-comments',
      path: "/91a709bb/",
    }, {"enable":true,"appId":"M7be75T70gn9gGNMraKahAvs-MdYXbMMI","appKey":"eg4W7uPbXRv7MmaSV1CpwACg","placeholder":"请在这里留言","avatar":"mp","meta":["nick","mail","link"],"pageSize":10,"lang":"zh-cn","visitor":true,"comment_count":true,"recordIP":true,"serverURLs":null,"enableQQ":true,"requiredFields":["nick"]}
    ));
  }, window.Valine);
});
</script>

</body>
</html>
